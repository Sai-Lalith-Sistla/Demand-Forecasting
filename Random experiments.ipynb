{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ce7931",
   "metadata": {},
   "source": [
    "#  Experimenting different Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b964e234",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sources for datasets\n",
    "\n",
    "- liftered from intresting git hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388a09ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9c6f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import math\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.arima_model import ARMA\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "from statsmodels.tsa.stattools import acf\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import PassiveAggressiveRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from scipy.stats import randint as sp_randint\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f32bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "get_ipython().run_line_magic(\"matplotlib\", \"inline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a9c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input():\n",
    "    forecast_period = input(\"What is the Future Forecast period? \")\n",
    "    forecast_period = int(forecast_period)\n",
    "\n",
    "    input_data = input(\"Enter dataset: \")\n",
    "\n",
    "\n",
    "    file_type = os.path.splitext(input_data)[1]\n",
    "    if file_type == '.csv':\n",
    "        dataset = pd.read_csv(input_data)\n",
    "\n",
    "    elif file_type == '.json':\n",
    "        dataset = retrieve_data(input_data)\n",
    "\n",
    "    return forecast_period, dataset\n",
    "\n",
    "def retrieve_data(path):\n",
    "    datasets = dict()\n",
    "    with open(path) as json_file:\n",
    "        raw_data = json.load(json_file)\n",
    "\n",
    "        for element in raw_data:\n",
    "            df_name = element['sku']\n",
    "            data = pd.DataFrame()\n",
    "            data['time'] = element['time']\n",
    "            data['sales'] = element['sales']\n",
    "            data['sales'] = [np.nan if pd.isnull(i) else int(i) for i in data['sales']]\n",
    "\n",
    "            data = data.T\n",
    "            data = data.rename(columns = data.iloc[0]).drop(data.index[0])\n",
    "            datasets[df_name] = copy.deepcopy(data)\n",
    "\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3145ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dickeyfullertest(series):\n",
    "    dftest = adfuller(series, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index= ['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "\n",
    "    for key, value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)' % key] = value\n",
    "\n",
    "    if dfoutput['p-value'] > 0.05:\n",
    "        return 0 #not stationary\n",
    "\n",
    "    else:\n",
    "        return 1 #stationary\n",
    "\n",
    "def mean_standard_deviation(dataset):\n",
    "    mu = np.mean(dataset.values)\n",
    "    sd = np.std(dataset.values)\n",
    "\n",
    "    ub = mu + (3 * sd)\n",
    "    lb = mu - (3* sd)\n",
    "\n",
    "    return lb,ub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac89aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def acf_plot1(dataset,freq):\n",
    "#    plot_acf(dataset)\n",
    "    res = acf(dataset)\n",
    "    ub=1.96/np.sqrt(len(dataset))\n",
    "    for i in range(1, len(res)-1):\n",
    "        if(res[i] > ub and res[i + 1] < ub):\n",
    "            p = i\n",
    "            if (p > freq):\n",
    "                p = freq\n",
    "            break\n",
    "    else:\n",
    "        p = freq\n",
    "    return p\n",
    "\n",
    "def acf_plot(dataset,freq):\n",
    "    res = acf(dataset)\n",
    "    plot_acf(dataset)\n",
    "    acfval=[0]*len(res)\n",
    "    ub=1.96/np.sqrt(len(dataset))\n",
    "    p1=1\n",
    "    for i in range(len(res)):\n",
    "        acfval[i]=abs(res[i])\n",
    "    acfval.sort(reverse=True)\n",
    "    acfval=np.array(acfval[1:])\n",
    "    pshort=np.array(acfval[0:3])\n",
    "    pshortind=[0]*len(pshort)\n",
    "#    print(pshort)\n",
    "    for i in range(len(pshort)):\n",
    "        pshortind[i]=np.where(abs(res)==pshort[i])[0][0]\n",
    "    ind=np.where(acfval>ub)[0]\n",
    "    finalacf=acfval[ind]\n",
    "    plist=[0]*len(finalacf)\n",
    "    for i in range(len(finalacf)):\n",
    "        plist[i]=np.where(abs(res)==finalacf[i])[0][0]\n",
    "#    return pshortind,plist\n",
    "#    print(plist)\n",
    "    while len(finalacf)>0:\n",
    "        p1=np.where(abs(res)==max(finalacf))[0][0]\n",
    "        if p1 > freq:\n",
    "            finalacf=finalacf[1:]\n",
    "        else:\n",
    "            break\n",
    "    return p1,pshortind,plist\n",
    "\n",
    "def pacf_plot1(dataset,freq):\n",
    "#    plot_pacf(dataset)\n",
    "    res = pacf(dataset)\n",
    "    ub=1.96/np.sqrt(len(dataset))\n",
    "    for i in range(1, len(res)-1):\n",
    "        if(res[i] > ub and res[i + 1] < ub):\n",
    "            q = i\n",
    "            if (q > freq/2):\n",
    "                q = freq/2\n",
    "            break\n",
    "    else:\n",
    "        q = freq/2\n",
    "    return int(q)\n",
    "\n",
    "def pacf_plot(dataset,freq):\n",
    "    res = pacf(dataset)\n",
    "    plot_pacf(dataset)\n",
    "    pacfval=[0]*len(res)\n",
    "    ub=1.96/np.sqrt(len(dataset))\n",
    "    q1=0\n",
    "    for i in range(len(res)):\n",
    "        pacfval[i]=abs(res[i])\n",
    "    pacfval.sort(reverse=True)\n",
    "    pacfval=np.array(pacfval[1:])\n",
    "    ind=np.where(pacfval>ub)[0]\n",
    "    finalpacf=pacfval[ind]\n",
    "    while len(finalpacf)>0:\n",
    "        q1=np.where(abs(res)==max(finalpacf))[0]\n",
    "        q1=q1[0]\n",
    "        if q1 > int(freq/2):\n",
    "            finalpacf=finalpacf[1:]\n",
    "        else:\n",
    "            break\n",
    "    return q1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42867f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Send alpha, beta\n",
    "def median_absolute_deviation(dataset, median):\n",
    "    median_list = list()\n",
    "    dataset.reset_index(drop = True, inplace = True)\n",
    "    for i in range(0, len(dataset)):\n",
    "        value = dataset.T[i] - median\n",
    "        median_list.append(value)\n",
    "    ms = np.abs(median_list)\n",
    "    mad = np.median(ms)\n",
    "    ub = median + (3 * mad)\n",
    "    lb = median - (3 * mad)\n",
    "\n",
    "\n",
    "    return ub,lb\n",
    "\n",
    "def dateformat(dataset):\n",
    "    dataset['time'] = dataset.index.tolist()\n",
    "    dataset['time']=pd.to_datetime(dataset['time'], format='%m/%d/%y',infer_datetime_format=True)\n",
    "    dataset.set_index('time', inplace = True)\n",
    "    return dataset\n",
    "\n",
    "def outlier_treatment(dataset):\n",
    "    median = np.median(dataset)\n",
    "    if median == 0:\n",
    "        ub,lb = mean_standard_deviation(dataset)\n",
    "    else:\n",
    "        ub,lb = median_absolute_deviation(dataset, median)\n",
    "    new_dataset = np.clip(dataset, lb, ub)\n",
    "    return new_dataset\n",
    "\n",
    "#pt--->outlier bucket size\n",
    "#sindex ---> number of zeros to categorize as sparse data\n",
    "#freq ---> seasonality\n",
    "def get_bucket_size(interval):\n",
    "    interval_type = find_interval_type(interval) #aggregation (weekly/monthly)\n",
    "    if interval_type == 'W':\n",
    "        pt=12\n",
    "        sindex=24\n",
    "        freq=52\n",
    "    elif interval_type=='M'or interval_type=='Random':\n",
    "        pt=6\n",
    "        sindex=10\n",
    "        freq=12\n",
    "    elif interval_type=='Y':\n",
    "        pt=2\n",
    "        sindex=0\n",
    "        freq=0\n",
    "    return pt,sindex,freq\n",
    "\n",
    "def outlier_treatment_tech(dataset,interval,pt): \n",
    "    start=0\n",
    "    end=pt\n",
    "    sku_data=[0]*len(dataset)\n",
    "    while end < len(dataset):\n",
    "        sku_data[start:end]=outlier_treatment(dataset[start:end])\n",
    "        start=end\n",
    "        end+=pt\n",
    "    if start < len(dataset):\n",
    "        sku_data[start:len(dataset)]=outlier_treatment(dataset[start:end])\n",
    "    sku_data=pd.DataFrame(sku_data)\n",
    "    return sku_data\n",
    "\n",
    "def Sesonal_detection(sku_data):\n",
    "    median = np.median(sku_data)\n",
    "\n",
    "    if median == 0:\n",
    "        ub,lb = mean_standard_deviation(sku_data)\n",
    "    else:\n",
    "        ub,lb = median_absolute_deviation(sku_data, median)\n",
    "    outliers1= sku_data > ub\n",
    "    outliers2 = sku_data < lb\n",
    "    a=np.where(outliers1==True)[0]\n",
    "    b=np.where(outliers2==True)[0]\n",
    "    flag1=flag2=1\n",
    "    if len(a)==0:\n",
    "        flag1=0\n",
    "        remove1=[]\n",
    "    if len(b)==0:\n",
    "        flag2=0\n",
    "        remove2=[]\n",
    "\n",
    "    if flag1==1:\n",
    "        k=np.zeros([len(a)-1, len(a)])\n",
    "        for i in range(0,(len(a)-1)):\n",
    "            for j in range(1,len(a)):\n",
    "                if a[j]==(a[i]+12) or a[j]==(a[i]+24):\n",
    "                    k[i][j]=1\n",
    "                else:\n",
    "                    k[i][j]=0\n",
    "        m=np.where(k!=0)\n",
    "        z=np.unique(m).tolist()\n",
    "        remove1=a[z]\n",
    "    if flag2==1:\n",
    "        q=np.zeros([len(b)-1, len(b)])\n",
    "        for i in range(0,(len(b)-1)):\n",
    "            for j in range(1,len(b)):\n",
    "                if b[j]==(b[i]+12) or b[j]==(b[i]+24):\n",
    "                    q[i][j]=1\n",
    "                else:\n",
    "                    q[i][j]=0\n",
    "        n=np.where(q!=0)\n",
    "        z1=np.unique(n).tolist()\n",
    "        remove2=b[z1]\n",
    "    return remove1,remove2,flag1,flag2\n",
    "\n",
    "def impute_missing_dates(dataset):\n",
    "    interval,start_date,end_date=find_interval(dataset.index)\n",
    "    drange = pd.date_range(start_date, end_date, freq = interval)\n",
    "    comp_data = pd.DataFrame(drange, columns = ['time'])\n",
    "    sales=dataset['sales'].to_dict()\n",
    "    comp_data['sales'] = comp_data['time'].map(sales)\n",
    "    comp_data.drop('time', axis = 1, inplace = True)\n",
    "    return comp_data, interval\n",
    "\n",
    "def find_interval(date):\n",
    "    date=pd.to_datetime(date, format='%m/%d/%Y',infer_datetime_format=True)\n",
    "    diff=[]\n",
    "    for i in range(len(date)-1):\n",
    "        interval=date[i+1]-date[i]\n",
    "        diff.append(interval)\n",
    "    mode=statistics.mode(diff)\n",
    "\n",
    "    return mode,date[0],date[-1]\n",
    "\n",
    "\n",
    "def find_interval_type(interval):\n",
    "    interval=interval.days\n",
    "    if interval==7:\n",
    "        itype='W'\n",
    "    elif interval==30 or interval==31:\n",
    "        itype='M'\n",
    "    elif interval==365:\n",
    "        itype='Y'\n",
    "    else:\n",
    "        itype='Random'\n",
    "\n",
    "    return itype\n",
    "\n",
    "def data_imputation_zero(dataset):\n",
    "    dataset.fillna(0, inplace = True)\n",
    "    return dataset\n",
    "\n",
    "#TODO: Send as transpose of dataset\n",
    "def data_imputation(dataset,freq):\n",
    "    #Taking the mean of nearest neighbours to fill NA\n",
    "\n",
    "    data_forward = dataset.fillna(method = 'ffill')\n",
    "    data_back = dataset.fillna(method = 'bfill')\n",
    "    data_back.fillna(0, inplace = True)\n",
    "    data_forward.fillna(0, inplace = True)\n",
    "\n",
    "    new_data = (data_forward.values + data_back.values) / 2\n",
    "    dataset=pd.DataFrame(dataset.values)\n",
    "\n",
    "    imput=dataset.isnull()\n",
    "    imput=imput[0]\n",
    "\n",
    "    dataset=dataset[0]\n",
    "    for i in range(len(dataset)):\n",
    "        div_factor = 3\n",
    "        if imput[i]==True:\n",
    "            #Negative index, set previous as 0\n",
    "            if i - freq < 0:\n",
    "                prev_value = 0\n",
    "                div_factor -= 1\n",
    "            #Fetch previous value\n",
    "            else:\n",
    "                prev_value = dataset[i - freq]\n",
    "\n",
    "            #Outside boundary or next value is NaN, set previous as 0\n",
    "            if i + freq >= len(dataset) or imput[i + freq]==True:\n",
    "                next_value = 0\n",
    "                div_factor -= 1\n",
    "            else:\n",
    "                next_value = dataset[i + freq]\n",
    "            dataset[i] = (new_data[i] + prev_value + next_value)/div_factor\n",
    "\n",
    "    df = pd.DataFrame(dataset)\n",
    "    return df\n",
    "\n",
    "#reading from first non-zero\n",
    "def read_from_first_sales(sku_data):\n",
    "    test=pd.isnull(sku_data)\n",
    "    index=np.where(test==False)[0]\n",
    "    index=index[0]\n",
    "    sku_data = sku_data[index:]\n",
    "    sku_data=sku_data.reset_index(drop = True)\n",
    "    return sku_data\n",
    "\n",
    "def new_forecast(data,forecast,forecast_period):\n",
    "\n",
    "    sample_data=data[-forecast_period:]\n",
    "    sample_data=pd.DataFrame(sample_data)\n",
    "    forecast=pd.DataFrame(forecast)\n",
    "    median_data=np.median(sample_data)\n",
    "    median_fore=np.median(forecast)\n",
    "    if median_data == 0:\n",
    "        ub_d,lb_d = mean_standard_deviation(sample_data)\n",
    "    else:\n",
    "        ub_d,lb_d = median_absolute_deviation(sample_data, median_data)\n",
    "    if median_fore == 0:\n",
    "        ub_f,lb_f = mean_standard_deviation(forecast)\n",
    "    else:\n",
    "        ub_f,lb_f = median_absolute_deviation(forecast, median_fore)\n",
    "    if ub_f >ub_d:\n",
    "        ub=ub_d\n",
    "    else:\n",
    "        ub=ub_f\n",
    "    if lb_f >lb_d:\n",
    "        lb=lb_d\n",
    "    else:\n",
    "        lb=lb_f\n",
    "    forecast=np.clip(forecast,lb,ub)\n",
    "    return forecast\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Function used to fit the model\n",
    "'''\n",
    "\n",
    "def fit_model(train_data, model):\n",
    "\n",
    "    X, y = train_data[:, 0:-1], train_data[:, -1]\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "'''\n",
    "Function used for one-step forecasting\n",
    "'''\n",
    "\n",
    "def forecast_model(model, X):\n",
    "\n",
    "    yhat = model.predict(X)\n",
    "\n",
    "    return yhat\n",
    "\n",
    "\n",
    "'''\n",
    "Function used for plotting\n",
    "\n",
    "'''\n",
    "def plotting(key, predictions, expected):\n",
    "    rmse = calculate_rmse(key, expected, predictions)\n",
    "    plt.figure()\n",
    "    plt.title(key)\n",
    "\n",
    "    y_values = list(expected) + list(predictions)\n",
    "    y_range = max(y_values) - min(y_values)\n",
    "    plt.text(6, min(y_values) + 0.2 * y_range, 'RMSE = ' + str(rmse))\n",
    "    plt.plot(predictions)\n",
    "    plt.plot(expected)\n",
    "    plt.legend(['predicted', 'expected'])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def calculate_mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    mape = sum(np.abs((y_true - y_pred) / y_true)* 100)/len(y_true)\n",
    "    if np.isnan(mape)== True or np.isfinite(mape)==False:\n",
    "        mape=0\n",
    "    return mape\n",
    "\n",
    "\n",
    "def calculate_facc(y_true, y_pred):\n",
    "\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    facc = 1 - (sum(np.abs(y_true - y_pred)) / sum(y_true))\n",
    "    if np.isnan(facc)== True or np.isfinite(facc)==False:\n",
    "        facc=0\n",
    "    return facc * 100\n",
    "\n",
    "\n",
    "\n",
    "def calculate_rmse(key, expected, predictions):\n",
    "    expected=np.array(expected)\n",
    "    rmse = sqrt(mean_squared_error(expected, predictions))\n",
    "    print(\"RMSE FOR %s: %d \" % (key, rmse))\n",
    "    return rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f3025b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbbed5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4f6d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba1b071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
